{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Machine Learning\n",
    "\n",
    "The most popular definitions of machine learning are the more general educational one\n",
    "\n",
    "\"The ability of a machine to learn without being explicitly programmed to do so\"\n",
    "\n",
    "and the more rigorous\n",
    "\n",
    "\"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance in task T, as measured by P, improves with experience E\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths of Machine Learning\n",
    "\n",
    "There are some problems in which ML specially excels:\n",
    "\n",
    "- **Expert systems**\n",
    "\n",
    "  There are some prediction problems, such as spam filtering or illness detection, that require a long list of fine-tuned rules to be predicted. That is, they would require a long list of hard-coded rules if they were to be solved using a traditional programming approach.\n",
    "\n",
    "- **Very complex problems**\n",
    "\n",
    "  Problems where there aren't any known algorithms that solve them, e.g. speech recognition.\n",
    "\n",
    "- **Fluctuating data**\n",
    "\n",
    "  Problems that rely on constantly uploading fluctuating data (spam filtering or illness detection, are examples of this too)\n",
    "\n",
    "- **Data mining**\n",
    "\n",
    "  Giving insights about a large complex dataset. This can be achieved by training a good model and inspecting it to see what it has learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ML:\n",
    "\n",
    "- **Supervised vs. Unsupervised** (and sel-supervised, semi-supervised, reinforcement, ...)\n",
    "\n",
    "  Supervised learning is the type of ML where each point in the dataset contains the correct answer, called target/label, which is the variable that you are trying to predict. There are 2 types of supervised learning: **classification**, where the target is a continuous variable, and **regression**, where the target is a categorical variable.\n",
    "\n",
    "  On the other hand, unsupervised learning is the type of ML that deals with unlabeled datasets. Some examples of unsupervised learning are clustering, anomaly detection, dimensionality reduction, and visualization algorithms.\n",
    "\n",
    "  Semi-supervised learning deals with partially labeled data, self-supervised learning generates a fully labeled dataset from an unsupervised one, and reinforcement learning trains an agent through rewards and penalties.\n",
    "\n",
    "- **Online vs. Batch**\n",
    "\n",
    "  If a model can learned incrementally from new data points, without having to be trained from scratch, then it is an example of online learning, else it is an example of batch / offline learning.\n",
    "\n",
    "  Online learning is superior when there is a need to quickly adapt to new data, when the dataset is huge and cannot fit on one machine's storage, or in the case that training the model on one go is too performance intensive (common in mobile devices). However, online learning has a unique challenge: the learning rate. The learning rate is the rate at which the model adapts to new data, and it must be fine-tuned. Also, online learning is sensitive to streaks of very poor incoming data (security attack, malfunctioning sensor, ...) and therefore it must be monitored.\n",
    "\n",
    "  Online learning has these additional challenges, and the models tend to be more complicated, therefore it's best to use batch learning if you can get away with it. In the case of batch learning it is important to take into account model rotting: the performance of the model WILL decay over time, simply because the model remains unchanged while the data always keeps evolving.\n",
    "\n",
    "- **Instance-based vs. Model-based**\n",
    "\n",
    "  This classifies models by the way in which they make predictions on new data points. On instance-based learning, the new data point is compared to the closest point or points in the training data, and the prediction is only based on that few similar points. In model-based learning, the whole dataset is used to optimize a function, which is later used to make predictions for new data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Challenges of Machine Learning\n",
    "\n",
    "- **Bad data**\n",
    "\n",
    "  **Not enough data.** ML models typically require a huge amount of data to perform well, even for simple tasks. More (good) data is always better, and it will significantly improve any model. A mediocre model can outperform a great one if it is given enough good quality data. However, not everything can be solved with more data because data is expensive and sometimes the dataset is finite (e.g. if you want to analyze shooting data in the NBA there are only so many shots that you can analyze).\n",
    "\n",
    "  **Poor quality data.** Sampling bias, bad measurements (errors->outliers, missing values, low precision->noise), irrelevant features.\n",
    "\n",
    "- **Bad model: overfitting and underfitting**\n",
    "\n",
    "  Overfitting happens when the model is too complex relative to the amount and noisiness of the data.\n",
    "  It can be solved by changing the dataset or the model.\n",
    "\n",
    "  To attempt to solve overfitting by changing the dataset you can reduce the number of features (feature selection: drooping the less relevant features, feature reduction, combining multiple features while preserving the most amount of information), gather more training data or reduce the noise in the training data (e.g. fix outliers).\n",
    "\n",
    "  Regularization is the process of constraining a model to solve overfitting. This is usually achieved by constraining the value of the parameters. The amount of regularization is controlled by a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and validating\n",
    "\n",
    "- **Test of the finished model**\n",
    "\n",
    "  The only way to now if your model will generalize to new data is to actually try it in new data. The best way to do this is to divide the dataset into the training and test sets. The generalization / out of sample error is the error of the model in the test set. It's the best way to test the accuracy of your model. The ratio of the size of the test and training sets is generally in the range of 10-20%. The larger the training set is, the lower this percentage can be. Applying your model to this test set is the last step, and it should only be done in one model, otherwise you would be cheating by fitting your model to the test set.\n",
    "\n",
    "- **Hyperparameter tunning and model selection**\n",
    "\n",
    "  When a model is fitted to a training set, its parameters are optimized so that it performs the best on that training set. How can you optimize the hyperparameters and model selection?\n",
    "\n",
    "  Let's divide the fraction of the dataset that isn't in the test set in 2 parts: the training and the dev set. Try out different hyperparameters and models in the train set, measure their performance in the dev set and select the best one.\n",
    "\n",
    "  If you try out too many models or hyperparameters you run the risk of overfitting the hyperparameter to that specific dev set, and it won't generalize as well to the test set. The best solution for this problem is cross-validation. In cross validation multiple training and dev sets are produced from the same original set, and the performance of a model is the average of its performance in the multiple sets. The only drawback of cross-validation is the training time, which is multiplied by the number of sets.\n",
    "\n",
    "In cases where there is an important data mismatch, that is when you are forced to use data for training that is not representative of your use case, there is an additional split of the data into a train-dev set (p36).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Free Lunch Theorem\n",
    "\n",
    "p37, paper by David Wolpert.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
